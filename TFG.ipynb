{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43abc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mplt\n",
    "import copy\n",
    "import csv\n",
    "from ESIOS import *\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def variables(fecha_inicio, fecha_fin):\n",
    "    token = '079272e1d20f8f1e79d1b886d49050d30534786e9ecc483b7835ff427a0d798f'\n",
    "    esios = ESIOS(token)\n",
    "    indicadores = [1, 12, 15, 20, 22, 24, 33, 36, 50, 57, 58, 59, 68, 71, 77, 79, 86, 89, 91, 92, 93, 96, 98, 105, 106, 107, 109, 113, 114, 117, 122, 123, 124, 127, 128, 129, 138, 141, 142, 147, 148, 152, 156, 157, 158, 160, 162, 173, 175, 176, 183, 184, 191, 192, 194, 197, 198, 201, 359, 360, 368, 371, 372, 373, 374, 380, 386, 387, 421, 454, 463, 480, 489, 492, 495, 496, 497, 498, 500, 502, 505, 510, 512, 514, 515, 516, 519, 522, 524, 528, 529, 530, 531, 532, 533, 534, 537, 538, 541, 572, 578, 579, 632, 633, 634, 674, 675, 680, 684, 685, 704, 709, 712, 726, 727, 734, 735, 738, 742, 750, 751, 754, 755, 757, 762, 764, 766, 770, 775, 776, 777, 778, 780, 781, 783, 784, 785, 786, 788, 789, 793, 794, 795, 797, 798, 802, 803, 804, 805, 806, 807, 813, 815, 816, 830, 843, 899, 1014, 1015, 1017, 1019, 1020, 1024, 1025, 1026, 1029, 1031, 1032, 1033, 1034, 1035, 1045, 1048, 1074, 1075, 1079, 1084, 1085, 1086, 1088, 1106, 1113, 1114, 1115, 1116, 1136, 1173, 1177, 1180, 1181, 1189, 1192, 1196, 1197, 1198, 1202, 1204, 1212, 1213, 1216, 1217, 1275, 1276, 1277, 1286, 1334, 1338, 1341, 1342, 1343, 1344, 1349, 1350, 1352, 1354, 1358, 1369, 1373, 1376, 1389, 1397, 1411, 1420, 1421, 1422, 1423, 1425, 1434, 1437, 1438, 1635, 1636, 1637, 1638, 1641, 1642, 1645, 1646, 1647, 1651, 1652, 1656, 1657, 1661, 1663, 1666, 1686, 1687, 1727, 1775, 1776, 1777, 1788, 10008, 10010, 10012, 10013, 10018, 10021, 10023, 10024, 10026, 10027, 10031, 10034, 10047, 10051, 10055, 10056, 10063, 10064, 10067, 10068, 10073, 10086, 10088, 10089, 10095, 10096, 10098, 10105, 10106, 10107, 10108, 10114, 10116, 10133, 10134, 10159, 10169, 10171, 10176, 10177, 10186, 10187, 10188, 10189, 10190, 10207, 10208, 10209, 10211, 10212, 10222, 10223, 10228, 10229, 10230, 10235, 10236, 10241, 10242, 10247, 10249, 10250, 10257, 10258, 10267, 10268, 10270, 10271, 10274, 10275, 10276, 10281, 10286, 10287, 10321, 10324, 10359, 10364, 10380, 10381]\n",
    "    lista = []\n",
    "    start = datetime.strptime(fecha_inicio, '%Y-%m-%d')\n",
    "    end = datetime.strptime(fecha_fin, '%Y-%m-%d')\n",
    "    dias = end - start\n",
    "    objetivo = dias.days*24+2  #Puede ser +1 +2 dependiendo del cambio horario\n",
    "    print('objetivo=' +str(objetivo))\n",
    "    for i in indicadores:\n",
    "        datos = esios.get_data(i, start, end)\n",
    "        if(datos is not None):\n",
    "            print(str(i) + '=' + str(len(datos)))\n",
    "            if(len(datos)==objetivo):\n",
    "                lista.append(i)\n",
    "    print('Tienes ' + str(len(lista)) + ' variables validas')\n",
    "    return lista\n",
    "\n",
    "def nombres_indicadores(indicadores):\n",
    "    token = '079272e1d20f8f1e79d1b886d49050d30534786e9ecc483b7835ff427a0d798f'\n",
    "    esios = ESIOS(token)\n",
    "    lista_nombres = esios.get_names(indicadores)\n",
    "    lista_final = dict(zip(lista_nombres,indicadores))\n",
    "    df = pd.DataFrame.from_dict(lista_final, orient='index')\n",
    "    df = df.transpose()\n",
    "    df.to_csv(\"nombres_indicadores.csv\", index=False)\n",
    "    print(lista_final) \n",
    "\n",
    "def extraer_datos(fecha_inicio, fecha_fin, nombre, indicadores = []):\n",
    "    token = '079272e1d20f8f1e79d1b886d49050d30534786e9ecc483b7835ff427a0d798f'\n",
    "    esios = ESIOS(token)\n",
    "    \n",
    "    start = datetime.strptime(fecha_inicio, '%Y-%m-%d')\n",
    "    end = datetime.strptime(fecha_fin, '%Y-%m-%d')\n",
    "   \n",
    "    if(not indicadores):\n",
    "\n",
    "        indicadores = variables(fecha_inicio, fecha_fin)\n",
    "    \n",
    "    df_list, names = esios.get_multiple_series(indicadores, start, end)\n",
    "    \n",
    "    final1 = esios.merge_series(df_list, names, pandas_sampling_interval='1H')\n",
    "    final1.to_csv(nombre+'.csv')\n",
    "\n",
    "def extraer_datos_por_partes(fecha_inicio, fecha_fin, nombre, indicadores = []):\n",
    "    corte = 5    #Número de variables en cada subgrupo\n",
    "    lista_indicadores = []\n",
    "    lista_df = []\n",
    "    i=0\n",
    "    e=0\n",
    " \n",
    "    if(not indicadores):\n",
    "        indicadores = variables(fecha_inicio, fecha_fin)\n",
    "\n",
    "    lista_indicadores=[indicadores[i:i + corte] for i in range(0, len(indicadores), corte)]\n",
    "    \n",
    "    if(len(lista_indicadores[-1])<corte):\n",
    "        lista_indicadores[-2]=lista_indicadores[-2]+lista_indicadores[-1]\n",
    "        lista_indicadores.pop()\n",
    "    print(lista_indicadores)\n",
    "    \n",
    "    for i in lista_indicadores:\n",
    "        texto = 'Extraer parte '+ str(e)\n",
    "        extraer_datos(fecha_inicio, fecha_fin, texto , i)\n",
    "        e=e+1\n",
    "        \n",
    "    for i in range(0,len(lista_indicadores)):\n",
    "        texto_leer = 'Extraer parte ' + str(i) + '.csv'\n",
    "        data = pd.read_csv(texto_leer)\n",
    "        if(i!=0):\n",
    "            del(data['datetime_utc'])\n",
    "        lista_df.append(data)  \n",
    "\n",
    "    df = pd.concat(lista_df,axis=1, join='inner')\n",
    "    pd.DataFrame.to_csv(df, nombre+'.csv', sep=',', na_rep='.', index=False)\n",
    "    print('Fin de extraer datos por partes')  \n",
    "    \n",
    "def seleccion_variables_pearson(nombre):\n",
    "    token = '079272e1d20f8f1e79d1b886d49050d30534786e9ecc483b7835ff427a0d798f'\n",
    "    esios = ESIOS(token)\n",
    "    df = pd.read_csv(nombre+\".csv\")\n",
    "    nombres = pd.read_csv(\"nombres_indicadores.csv\")\n",
    "    columnas = df.columns\n",
    "    objetivo = df['Precio medio horario final suma de componentes']\n",
    "    lista04 = []\n",
    "    lista06 = []\n",
    "    lista075 = []\n",
    "    for nom in columnas:\n",
    "        if(not nom=='datetime_utc'):\n",
    "            columna = df[nom]\n",
    "            col = pd.to_numeric(columna)\n",
    "            corr, _ = pearsonr(objetivo, columna)\n",
    "            lista_final.append('%.3f' % corr+' : '+nom)\n",
    "            if(abs(corr) >= 0.75):\n",
    "                ind = nombres.get(nom)\n",
    "                lista075.append(int(ind[0]))\n",
    "                lista06.append(int(ind[0]))\n",
    "                lista04.append(int(ind[0]))\n",
    "            elif(abs(corr) >= 0.6):\n",
    "                ind = nombres.get(nom)\n",
    "                lista06.append(int(ind[0]))\n",
    "                lista04.append(int(ind[0]))\n",
    "            elif(abs(corr) >= 0.4):\n",
    "                ind = nombres.get(nom)\n",
    "                lista04.append(int(ind[0]))     \n",
    "    return lista075, lista06, lista04\n",
    "\n",
    "def MAE (test_y, predicciones):\n",
    "    resultados=[]\n",
    "    for i in range(0, len(test_y)):\n",
    "        resultados.append(abs(test_y[i]-predicciones[i]))\n",
    "    return np.mean(resultados)\n",
    "\n",
    "def MAPE (test_y, predicciones):\n",
    "    resultados=[]\n",
    "    for i in range(0, len(test_y)):\n",
    "        resultados.append(abs(test_y[i]-predicciones[i])/test_y[i]*100)\n",
    "    return np.mean(resultados)\n",
    "\n",
    "def WAPE (test_y, predicciones): \n",
    "    actual=sum(test_y)\n",
    "    pred=sum(predicciones)   \n",
    "    return float(abs(actual-pred)/actual*100)\n",
    "\n",
    "def resultado(nombre, algoritmo, num):\n",
    "    MAE=[]\n",
    "    MAPE=[]\n",
    "    WAPE=[]\n",
    "    for i in range(0,num):\n",
    "        print(\"Iteración \"+str(i+1))\n",
    "        resultadoMAE,resultadoMAPE,resultadoWAPE=algoritmo(nombre)\n",
    "        MAE.append(resultadoMAE)\n",
    "        MAPE.append(resultadoMAPE)\n",
    "        WAPE.append(resultadoWAPE)\n",
    "    resultado=[np.mean(MAE),np.mean(MAPE),np.mean(WAPE)]\n",
    "    print(\"================================\")\n",
    "    print(\"Media final: \\nError MAE: \"+str(resultado[0])+\"\\nError MAPE: \"+str(resultado[1])+\"\\nError WAPE: \"+str(resultado[2]))\n",
    "    return resultado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraer_datos_por_partes('2018-10-15', '2019-01-13', 'Primer periodo')\n",
    "lista075, lista06, lista04 = seleccion_variables_pearson('Primer periodo')\n",
    "extraer_datos_por_partes('2018-10-15', '2019-01-13', 'Primer periodo 075', lista075)\n",
    "extraer_datos_por_partes('2018-10-15', '2019-01-13', 'Primer periodo 06', lista06)\n",
    "extraer_datos_por_partes('2018-10-15', '2019-01-13', 'Primer periodo 04', lista04)\n",
    "\n",
    "extraer_datos_por_partes('2020-10-15', '2021-01-13', 'Segundo periodo')\n",
    "lista075, lista06, lista04 = seleccion_variables_pearson('Segundo periodo')\n",
    "extraer_datos_por_partes('2020-10-15', '2021-01-13', 'Segundo periodo 075', lista075)\n",
    "extraer_datos_por_partes('2020-10-15', '2021-01-13', 'Segundo periodo 06', lista06)\n",
    "extraer_datos_por_partes('2020-10-15', '2021-01-13', 'Segundo periodo 04', lista04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327195c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def decision_tree(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\")\n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "    \n",
    "    modelo = DecisionTreeRegressor(criterion='mae')\n",
    "    modelo.fit(train_X, train_y)\n",
    "\n",
    "    predicciones = modelo.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE\n",
    "    \n",
    "def multilayer_perceptron(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\") \n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "    \n",
    "    modelo = MLPRegressor(hidden_layer_sizes=(32,32,32,32,32), activation=\"relu\", random_state=1, max_iter=2000)\n",
    "    modelo.fit(train_X, train_y)\n",
    "\n",
    "    predicciones = modelo.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE\n",
    "\n",
    "def lstm(nombre): \n",
    "    dataset = pd.read_csv(nombre+ \".csv\", header=0, index_col=0)\n",
    "    values = dataset.values\n",
    "    names=dataset.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division= len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "    modelo = Sequential()\n",
    "    modelo.add(LSTM(500, activation='linear', use_bias=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    modelo.add(Dense(1))\n",
    "    modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_absolute_error')\n",
    "\n",
    "    history = modelo.fit(train_X, train_y, epochs=150, batch_size=100, validation_data=(test_X, test_y), \n",
    "                         verbose=2, shuffle=False, validation_split=0.2)\n",
    "\n",
    "    predicciones=modelo.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(\"Segundo periodo 075\")\n",
    "multilayer_perceptron(\"Segundo periodo 075\")\n",
    "lstm(\"Segundo periodo 075\")\n",
    "\n",
    "resultado(\"Primer periodo 06\",decision_tree,20)\n",
    "resultado(\"Primer periodo 06\",multilayer_perceptron,20)\n",
    "resultado(\"Primer periodo 06\",lstm,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f0127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hyperband import HyperbandSearchCV\n",
    "\n",
    "def decision_tree_gr(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\") \n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "    \n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = {'criterion': [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "                'splitter': ['best', 'random'],\n",
    "                'max_depth': [None, 10, 100, 1000, 2000],\n",
    "                'max_leaf_nodes': [100 ,500, 1000, 2000],\n",
    "                'min_samples_split': [2, 4, 8]}\n",
    "\n",
    "    gsc = GridSearchCV(estimator, param_grid, cv=2, verbose=3, n_jobs=4)\n",
    "\n",
    "    grid_result = gsc.fit(train_X, train_y)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "\n",
    "    best_dt = DecisionTreeRegressor(criterion=best_params[\"criterion\"],splitter=best_params[\"splitter\"],\n",
    "                                    max_depth=best_params[\"max_depth\"], max_leaf_nodes=best_params[\"max_leaf_nodes\"],\n",
    "                                    min_samples_split=best_params[\"min_samples_split\"])\n",
    "    \n",
    "    best_dt.fit(train_X, train_y)\n",
    "   \n",
    "    print(\"criterion: \"+str(best_params.get('criterion')))\n",
    "    print(\"splitter: \"+str(best_params.get('splitter')))\n",
    "    print(\"max_depth: \"+str(best_params.get('max_depth')))\n",
    "    print(\"max_leaf_nodes: \"+str(best_params.get('max_leaf_nodes')))\n",
    "    print(\"min_samples_split: \"+str(best_params.get('min_samples_split')))\n",
    "    \n",
    "    predicciones = best_dt.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "\n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "\n",
    "    return errorMAE,errorMAPE,errorWAPE\n",
    "\n",
    "def decision_tree_hb(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\") \n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "\n",
    "    estimator = DecisionTreeRegressor()\n",
    "\n",
    "    param_grid = {'criterion': [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "                'splitter': ['best', 'random'],\n",
    "                'max_depth': [None, 10, 100, 1000, 2000],\n",
    "                'max_leaf_nodes': [100 ,500, 1000, 2000],\n",
    "                'min_samples_split': [2, 4, 8]}\n",
    "\n",
    "    gsc = HyperbandSearchCV(estimator=estimator, param_distributions=param_grid, \n",
    "                        resource_param='min_samples_split', scoring='neg_mean_absolute_error', \n",
    "                        verbose=1, max_iter=500)\n",
    "\n",
    "    grid_result = gsc.fit(train_X, train_y)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "\n",
    "    best_dt = DecisionTreeRegressor(criterion = best_params[\"criterion\"], splitter = best_params[\"splitter\"],\n",
    "                        max_depth = best_params[\"max_depth\"],\n",
    "                        max_leaf_nodes = best_params[\"max_leaf_nodes\"], min_samples_split = best_params[\"min_samples_split\"])\n",
    "    \n",
    "    best_dt.fit(train_X, train_y)\n",
    "    \n",
    "    print(\"criterion: \"+str(best_params.get('criterion')))\n",
    "    print(\"splitter: \"+str(best_params.get('splitter')))\n",
    "    print(\"max_depth: \"+str(best_params.get('max_depth')))\n",
    "    print(\"max_leaf_nodes: \"+str(best_params.get('max_leaf_nodes')))\n",
    "    print(\"min_samples_split: \"+str(best_params.get('min_samples_split')))\n",
    "    \n",
    "    predicciones = best_dt.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4117334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "inicio1 = datetime.now()\n",
    "decision_tree(\"Segundo periodo 04\")\n",
    "fin1 = datetime.now()\n",
    "duracion = fin1 - inicio1\n",
    "print('Durancion: ' + str(duracion))\n",
    "\n",
    "inicio2 = datetime.now()\n",
    "decision_tree_gr(\"Segundo periodo 04\")\n",
    "fin2 = datetime.now()\n",
    "duracion = fin2 - inicio2\n",
    "print('Durancion: ' + str(duracion))\n",
    "\n",
    "inicio3 = datetime.now()\n",
    "decision_tree_hb(\"Segundo periodo 04\")\n",
    "fin3 = datetime.now()\n",
    "duracion = fin3 - inicio3\n",
    "print('Durancion: ' + str(duracion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb22cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hyperband import HyperbandSearchCV\n",
    "\n",
    "def multilayer_perceptron_gr(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\") \n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "\n",
    "    estimator=MLPRegressor()\n",
    "\n",
    "    param_grid = {'hidden_layer_sizes': [(32,32,32,32,32), (64,64,64,64,64), (100,100,100), (75, 75, 75, 75), (50,100,50), (100,)],\n",
    "          'activation': ['relu', 'tanh', 'logistic', 'identity'],\n",
    "          'alpha': [0.0001, 0.001, 0.01],\n",
    "          'solver': ['lbfgs'],\n",
    "          'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "          'shuffle': [True, False],\n",
    "          'batch_size': ['auto'],\n",
    "          'max_iter': [500]}\n",
    "\n",
    "    gsc = GridSearchCV(estimator, param_grid, cv=2, scoring='neg_mean_squared_error', verbose=3, n_jobs=4)\n",
    "\n",
    "    grid_result = gsc.fit(train_X, train_y)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "\n",
    "    best_mlp = MLPRegressor(hidden_layer_sizes = best_params[\"hidden_layer_sizes\"], \n",
    "                        activation =best_params[\"activation\"],\n",
    "                        alpha = best_params[\"alpha\"],\n",
    "                        learning_rate=best_params[\"learning_rate\"],\n",
    "                        solver='lbfgs',\n",
    "                        shuffle=best_params[\"shuffle\"],\n",
    "                        batch_size='auto',\n",
    "                        max_iter=1000)\n",
    "    \n",
    "    best_mlp.fit(train_X, train_y)\n",
    "    \n",
    "    print(\"hidden_layer_sizes: \"+str(best_params.get('hidden_layer_sizes')))\n",
    "    print(\"activation: \"+str(best_params.get('activation')))\n",
    "    print(\"alpha: \"+str(best_params.get('alpha')))\n",
    "    print(\"learning_rate: \"+str(best_params.get('learning_rate')))\n",
    "    print(\"solver: \"+str(best_params.get('solver')))\n",
    "    print(\"shuffle: \"+str(best_params.get('shuffle')))\n",
    "    print(\"batch_size: \"+str(best_params.get('batch_size')))\n",
    "    \n",
    "    predicciones = best_mlp.predict(test_X)\n",
    "    \n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE\n",
    "\n",
    "def multilayer_perceptron_hb(nombre):\n",
    "    df = pd.read_csv(nombre+\".csv\",index_col=\"datetime_utc\") \n",
    "    values = df.values\n",
    "    names=df.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division = len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "\n",
    "    estimator=MLPRegressor()\n",
    "\n",
    "    param_grid = {'hidden_layer_sizes': [(32,32,32,32,32), (64,64,64,64,64), (100,100,100), (75, 75, 75, 75), (50,100,50), (100,)],\n",
    "          'activation': ['relu', 'tanh', 'logistic', 'identity'],\n",
    "          'alpha': [0.0001, 0.001, 0.01],\n",
    "          'solver': ['lbfgs'],\n",
    "          'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "          'shuffle': [True, False],\n",
    "          'batch_size': ['auto'],\n",
    "          'max_iter': [500],\n",
    "          'n_iter_no_change': [10, 20, 30]}\n",
    "\n",
    "    gsc = HyperbandSearchCV(estimator=estimator, cv=3, param_distributions=param_grid,resource_param='n_iter_no_change', scoring='neg_mean_squared_error',verbose=3,n_jobs=4,max_iter=500)\n",
    "\n",
    "    grid_result = gsc.fit(train_X, train_y)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "\n",
    "    best_mlp = MLPRegressor(hidden_layer_sizes = best_params[\"hidden_layer_sizes\"], \n",
    "                        activation =best_params[\"activation\"],\n",
    "                        alpha = best_params[\"alpha\"],\n",
    "                        learning_rate=best_params[\"learning_rate\"],\n",
    "                        solver='lbfgs',\n",
    "                        shuffle=best_params[\"shuffle\"],\n",
    "                        batch_size='auto',\n",
    "                        n_iter_no_change=best_params[\"n_iter_no_change\"],\n",
    "                        max_iter=10000)\n",
    "    \n",
    "    best_mlp.fit(train_X, train_y)\n",
    "    \n",
    "    print(\"hidden_layer_sizes: \"+str(best_params.get('hidden_layer_sizes')))\n",
    "    print(\"activation: \"+str(best_params.get('activation')))\n",
    "    print(\"alpha: \"+str(best_params.get('alpha')))\n",
    "    print(\"learning_rate: \"+str(best_params.get('learning_rate')))\n",
    "    print(\"shuffle: \"+str(best_params.get('shuffle')))\n",
    "    \n",
    "    predicciones = best_mlp.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "    \n",
    "    return errorMAE,errorMAPE,errorWAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c6939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "inicio1 = datetime.now()\n",
    "multilayer_perceptron(\"Segundo periodo 04\")\n",
    "fin1 = datetime.now()\n",
    "duracion = fin1 - inicio1\n",
    "print('Durancion: ' + str(duracion))\n",
    "\n",
    "inicio2 = datetime.now()\n",
    "multilayer_perceptron_gr(\"Segundo periodo 04\")\n",
    "fin2 = datetime.now()\n",
    "duracion = fin2 - inicio2\n",
    "print('Durancion: ' + str(duracion))\n",
    "\n",
    "inicio3 = datetime.now()\n",
    "multilayer_perceptron_hb(\"Segundo periodo 04\")\n",
    "fin3 = datetime.now()\n",
    "duracion = fin3 - inicio3\n",
    "print('Durancion: ' + str(duracion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08dc7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from matplotlib import pyplot\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def data():\n",
    "    dataset = pd.read_csv(\"Segundo periodo 075.csv\", header=0, index_col=0)\n",
    "    values = dataset.values\n",
    "    names=dataset.columns\n",
    "    indice=names.get_loc(\"Precio medio horario final suma de componentes\")\n",
    "    values = values.astype('float32')\n",
    "\n",
    "    division= len(values)*66//100\n",
    "    values = sklearn.utils.shuffle(values)\n",
    "    train = values[:division, :]\n",
    "    test = values[division:, :]\n",
    "\n",
    "    train_y=train[:, indice]\n",
    "    train = np.delete(train, indice, 1) \n",
    "    train_X= train[:, :]\n",
    "    test_y=test[:, indice]\n",
    "    test = np.delete(test, indice, 1) \n",
    "    test_X = test[:, :]\n",
    "\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "  \n",
    "def lstm_gr(nombre): \n",
    "    \n",
    "    train_X, test_X, train_y, test_y = data()\n",
    "    \n",
    "    estimator=KerasRegressor(model_builder_gr)\n",
    "    \n",
    "    param_grid = {'gr_units': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "          'gr_activation': ['tanh', 'linear', 'relu'],\n",
    "          'gr_use_bias': [True, False],\n",
    "          'gr_learning_rate': [0.0001, 0.001, 0.01], \n",
    "          'gr_capas':[1, 2, 3]}\n",
    "    \n",
    "    gsc = GridSearchCV(estimator, param_grid, cv=2, scoring='neg_mean_squared_error', verbose=3, n_jobs=5)\n",
    "    \n",
    "    grid_result = gsc.fit(train_X, train_y)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    if best_params.get('gr_capas') == 1:\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                    use_bias=best_params.get('gr_use_bias'),\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    elif best_params.get('gr_capas') == 2:\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                    use_bias=best_params.get('gr_use_bias'), return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                        use_bias=best_params.get('gr_use_bias')))\n",
    "    else:\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                    use_bias=best_params.get('gr_use_bias'), return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                    use_bias=best_params.get('gr_use_bias'), return_sequences=True))\n",
    "        modelo.add(LSTM(units=best_params.get('gr_units'), activation=best_params.get('gr_activation'), \n",
    "                    use_bias=best_params.get('gr_use_bias')))\n",
    "    modelo.add(Dense(1))\n",
    "    modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params.get('gr_learning_rate')), \n",
    "                   loss='mean_absolute_error')\n",
    "\n",
    "    history = modelo.fit(train_X, train_y, epochs=150, batch_size=100, validation_data=(test_X, test_y), \n",
    "                         verbose=2, shuffle=False, validation_split=0.2)\n",
    "\n",
    "    print(\"units: \"+str(best_params.get('gr_units')))\n",
    "    print(\"activation: \"+str(best_params.get('gr_activation')))\n",
    "    print(\"use_bias: \"+str(best_params.get('gr_use_bias')))\n",
    "    print(\"learning_rate: \"+str(best_params.get('gr_learning_rate')))\n",
    "    print(\"capas: \"+str(best_params.get('gr_capas')))\n",
    "    \n",
    "    predicciones=modelo.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "\n",
    "def lstm_hb(nombre): \n",
    "    \n",
    "    train_X, test_X, train_y, test_y = data()\n",
    "    \n",
    "    tuner = Hyperband(model_builder,\n",
    "                     objective='loss',\n",
    "                     factor=5,\n",
    "                     max_epochs=100,\n",
    "                     hyperband_iterations=1,\n",
    "                     directory='prueba',\n",
    "                     project_name='lstm')\n",
    "    \n",
    "    stop_early = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    tuner.search(train_X, train_y, epochs=150, validation_data=(test_X, test_y),callbacks=[stop_early])\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "    \n",
    "    print(\"units: \"+str(best_hps.get('units')))\n",
    "    print(\"learning_rate: \"+str(best_hps.get('learning_rate')))\n",
    "    print(\"activation: \"+str(best_hps.get('activation')))\n",
    "    print(\"use_bias: \"+str(best_hps.get('use_bias')))\n",
    "    print(\"capas: \"+str(best_hps.get('capas')))\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    if best_hps.get('capas') == 1:\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                    use_bias=best_hps.get('use_bias'),\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    elif best_hps.get('capas') == 2:\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                    use_bias=best_hps.get('units'), return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                        use_bias=best_hps.get('use_bias')))\n",
    "    else:\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                    use_bias=best_hps.get('use_bias'), return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                    use_bias=best_hps.get('use_bias'), return_sequences=True))\n",
    "        modelo.add(LSTM(units=best_hps.get('units'), activation=best_hps.get('activation'), \n",
    "                    use_bias=best_hps.get('use_bias')))\n",
    "    modelo.add(Dense(1))\n",
    "    modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')), \n",
    "                   loss='mean_absolute_error')\n",
    "\n",
    "    history = modelo.fit(train_X, train_y, epochs=150, batch_size=100, validation_data=(test_X, test_y), \n",
    "                         verbose=2, shuffle=False, validation_split=0.2)\n",
    "    \n",
    "    predicciones=modelo.predict(test_X)\n",
    "    errorMAE=MAE(test_y, predicciones)\n",
    "    print(\"Error MAE: \"+str(errorMAE))\n",
    "     \n",
    "    errorMAPE=MAPE(test_y, predicciones)\n",
    "    print(\"Error MAPE: \"+str(errorMAPE))\n",
    "\n",
    "    errorWAPE=WAPE(test_y, predicciones)\n",
    "    print(\"Error WAPE: \"+str(errorWAPE))\n",
    "\n",
    "def model_builder(hp):\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = data()\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=200, max_value=2000, step=200)\n",
    "    hp_activation = hp.Choice('activation', values=['tanh', 'linear', 'relu'])\n",
    "    hp_use_bias = hp.Choice('use_bias', values=[False, True])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_capas = hp.Choice('capas', values=[1, 2, 3])\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    \n",
    "    if hp_capas == 1:\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    elif hp_capas == 2:\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias, return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias))\n",
    "    else:\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias, return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias, return_sequences=True))\n",
    "        modelo.add(LSTM(units=hp_units, activation=hp_activation, use_bias=hp_use_bias))\n",
    "\n",
    "    modelo.add(Dense(1))\n",
    "    modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='mean_absolute_error')\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "def model_builder_gr(gr_units, gr_activation, gr_use_bias, gr_learning_rate, gr_capas):\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = data()\n",
    "    \n",
    "    modelo = Sequential()\n",
    "    \n",
    "    if gr_capas == 1:\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    elif gr_capas == 2:\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias, return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias))\n",
    "    else:\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias, return_sequences=True,\n",
    "                    input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias, return_sequences=True))\n",
    "        modelo.add(LSTM(units=gr_units, activation=gr_activation, use_bias=gr_use_bias))\n",
    "    \n",
    "    modelo.add(Dense(1))\n",
    "    modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=gr_learning_rate), loss='mean_absolute_error')\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3dcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "inicio1 = datetime.now()\n",
    "lstm(\"Segundo periodo 04\")\n",
    "fin1 = datetime.now()\n",
    "duracion = fin1 - inicio1\n",
    "print('Durancion: ' + str(duracion))\n",
    "\n",
    "inicio2 = datetime.now()\n",
    "lstm_gr(\"Segundo periodo 04\")\n",
    "fin2 = datetime.now()\n",
    "duracion = fin2 - inicio2\n",
    "print('Durancion: ' + str(duracion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d535844",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hb(\"Segundo periodo 04\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
